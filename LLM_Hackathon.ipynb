{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 11509022,
          "sourceType": "datasetVersion",
          "datasetId": 7216441
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "name": "notebook596c3fbf27"
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "jagapathi44_llm_dataset_path = kagglehub.dataset_download('jagapathi44/llm-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "b18PUYYGLwEV"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "jagapathi44_llm_dataset_path = kagglehub.dataset_download('jagapathi44/llm-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "BLDXLS4FGKMZ",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T10:52:18.280432Z",
          "iopub.execute_input": "2025-04-22T10:52:18.280628Z",
          "iopub.status.idle": "2025-04-22T10:52:18.71973Z",
          "shell.execute_reply.started": "2025-04-22T10:52:18.280602Z",
          "shell.execute_reply": "2025-04-22T10:52:18.719069Z"
        },
        "outputId": "4aec4c99-693c-448d-dcc5-2f7640be2997"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Data source import complete.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T10:52:23.070824Z",
          "iopub.execute_input": "2025-04-22T10:52:23.071418Z",
          "iopub.status.idle": "2025-04-22T10:52:24.925116Z",
          "shell.execute_reply.started": "2025-04-22T10:52:23.071392Z",
          "shell.execute_reply": "2025-04-22T10:52:24.924365Z"
        },
        "id": "A0PTHXA_GKMb",
        "outputId": "4b90dee1-3957-4fa8-bedd-f2caa659022b"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/stable_diffusion/19853_shylaja.sharath_31_20250327092700214_Video_ENC (1).mp4\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/stable_diffusion/Stable Diffusion.pdf\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/stable_diffusion/19853_shylaja.sharath_31_20250327084200249_Video_ENC.mp4\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/expn_tree/7b_2020-09-25 12-12-37_ExprTReeCode 00_00_09-00_26_52.mkv\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/expn_tree/Class7_Unit3_Trees_ExprTree.pptx\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/expn_tree/7a_2020-09-24 09-28-52_ExprTreeCon.mkv\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/agentic/19853_shylaja.sharath_31_20250401121200417_Video_ENC.mp4\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/agentic/AutoGen, CrewAI.pdf\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/agentic/Agentic Workflow.pdf\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Lora_Qlora/19853_shylaja.sharath_31_20250318125700082_Video_ENC.mp4\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Lora_Qlora/Finetuning.pdf\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Lora_Qlora/19853_shylaja.sharath_31_20250318121200085_Video_ENC (1).mp4\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Lora_Qlora/19853_shylaja.sharath_31_20250318112700094_Video_ENC (1).mp4\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Heap/8a_2020-09-24 13-07-04_HeapCon 00_00_01-00_42_12~1.mkv\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Heap/Class8_Unit3_Trees_Heap.pptx\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Heap/8b_2020-09-26 09-34-27_HeapCode 00_00_09-00_56_50.mkv\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/BST/Class2_Unit3_Tree_BST_DynamicInsert.pptx\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/BST/Class3_Unit3_Trees_BSTDeletion.pptx\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/BST/Class4_Unit3_Trees_BST_ArrayInsert.pptx\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/TBT/Class6_Unit3_Trees_ThreadBST.pptx\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/TBT/6b_2020-09-25 11-29-19_TBTCode 00_00_04-00_35_19~8.mkv\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/TBT/6a_2020-09-22 09-49-32_TBTCon.mkv\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Multimodal/19853_shylaja.sharath_31_20250401112700078_Video_ENC.mp4\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Multimodal/MAMBA.pdf\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Multimodal/MultiModal LLMs.pdf\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/binary_tree_traversal/5a_2020-09-15 09-04-51_BinTraversal.mkv\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/binary_tree_traversal/Class5_Unit3_BST_Traversal.pptx\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Tree_traversal/class9_Unit3_Trees_naryTraversal.pptx\n/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Tree_traversal/9a_2020-09-16 09-46-13_TreeTravCon.mkv\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-pptx fpdf python-dotenv ffmpeg-python transformers pypdf requests"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T10:52:29.022516Z",
          "iopub.execute_input": "2025-04-22T10:52:29.022908Z",
          "iopub.status.idle": "2025-04-22T10:52:35.61387Z",
          "shell.execute_reply.started": "2025-04-22T10:52:29.022884Z",
          "shell.execute_reply": "2025-04-22T10:52:35.613181Z"
        },
        "id": "pTeMC6fvGKMc",
        "outputId": "4d58bb3e-8cdd-4ebe-b154-59cfcceb290d"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting python-pptx\n  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\nCollecting fpdf\n  Downloading fpdf-1.7.2.tar.gz (39 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting python-dotenv\n  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\nCollecting ffmpeg-python\n  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\nRequirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (11.1.0)\nCollecting XlsxWriter>=0.5.7 (from python-pptx)\n  Downloading XlsxWriter-3.2.3-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (5.3.1)\nRequirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-pptx) (4.13.1)\nRequirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ffmpeg-python) (1.0.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\nDownloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\nDownloading XlsxWriter-3.2.3-py3-none-any.whl (169 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: fpdf\n  Building wheel for fpdf (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for fpdf: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40704 sha256=49ae52ee46be00d653c5d3dcaddf180f9a0f62fcfa48db0486be34fc1541e965\n  Stored in directory: /root/.cache/pip/wheels/65/4f/66/bbda9866da446a72e206d6484cd97381cbc7859a7068541c36\nSuccessfully built fpdf\nInstalling collected packages: fpdf, XlsxWriter, python-dotenv, ffmpeg-python, python-pptx\nSuccessfully installed XlsxWriter-3.2.3 ffmpeg-python-0.2.0 fpdf-1.7.2 python-dotenv-1.1.0 python-pptx-1.0.2\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import ffmpeg\n",
        "import json\n",
        "import time\n",
        "from pypdf import PdfReader\n",
        "from transformers import pipeline\n",
        "from pptx import Presentation\n",
        "import requests\n",
        "from fpdf import FPDF\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML, Markdown\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "import google.generativeai as genai"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T10:52:38.521363Z",
          "iopub.execute_input": "2025-04-22T10:52:38.521609Z",
          "iopub.status.idle": "2025-04-22T10:53:05.312694Z",
          "shell.execute_reply.started": "2025-04-22T10:52:38.52159Z",
          "shell.execute_reply": "2025-04-22T10:53:05.31211Z"
        },
        "id": "rc6LA_0eGKMd",
        "outputId": "42edba20-8463-4b24-e410-1e84d1649f05"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-04-22 10:52:49.128610: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745319169.320059      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745319169.374307      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "asr = pipeline(\n",
        "    task=\"automatic-speech-recognition\",\n",
        "    model=\"openai/whisper-small\",\n",
        "    chunk_length_s=30,\n",
        "    stride_length_s=5,\n",
        "    device=0\n",
        ")\n",
        "\n",
        "# Set your API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyBfag2nmu6cxMcwBBOdCy73Qx4VVP2QFSE\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T10:53:11.520956Z",
          "iopub.execute_input": "2025-04-22T10:53:11.521811Z",
          "iopub.status.idle": "2025-04-22T10:53:19.756585Z",
          "shell.execute_reply.started": "2025-04-22T10:53:11.521784Z",
          "shell.execute_reply": "2025-04-22T10:53:19.756017Z"
        },
        "id": "o9HmVYOcGKMd",
        "outputId": "aeabdc83-60b0-4919-be2b-5d5394d9e6db",
        "colab": {
          "referenced_widgets": [
            "380536acff5a4f48ab73ae441b69aada",
            "220cf59c769d4e9281e05cbb91c98ae9",
            "9166dcd14021402f8c0e6869ed13bc69",
            "846276d07f5b46ac9808897fe4930254",
            "4f4f94edb29c46f284f733f040b7c029",
            "521058457a8a428ebe756c4519605302",
            "8d537bb3b9304590948df44498b3fde9",
            "42b2ef66e601440fb155e696dd5e4266",
            "226b31e3fd4e4945b91f01ae42a99c0a",
            "5266725f1e984b7ca5b667d8d20a7ce6",
            "6559b0b147b540ccbefbfef9ed5b4d07"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/1.97k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "380536acff5a4f48ab73ae441b69aada"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "220cf59c769d4e9281e05cbb91c98ae9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/3.87k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9166dcd14021402f8c0e6869ed13bc69"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "846276d07f5b46ac9808897fe4930254"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f4f94edb29c46f284f733f040b7c029"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "521058457a8a428ebe756c4519605302"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d537bb3b9304590948df44498b3fde9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42b2ef66e601440fb155e696dd5e4266"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "226b31e3fd4e4945b91f01ae42a99c0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5266725f1e984b7ca5b667d8d20a7ce6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6559b0b147b540ccbefbfef9ed5b4d07"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Device set to use cuda:0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_pdf_text(pdf_path):\n",
        "    \"\"\"Extracts and returns all text from a PDF file.\"\"\"\n",
        "    reader = PdfReader(pdf_path)\n",
        "    texts = [page.extract_text() or \"\" for page in reader.pages]\n",
        "    return \"\\n\".join(texts)\n",
        "\n",
        "# Function to extract content from PowerPoint presentations\n",
        "def extract_pptx_content(pptx_path):\n",
        "    \"\"\"Extracts structured content from PowerPoint presentations.\"\"\"\n",
        "    if not os.path.exists(pptx_path):\n",
        "        print(f\"Warning: PowerPoint file not found: {pptx_path}\")\n",
        "        return \"\"\n",
        "\n",
        "    presentation = Presentation(pptx_path)\n",
        "    content = []\n",
        "\n",
        "    # Process each slide\n",
        "    for i, slide in enumerate(presentation.slides):\n",
        "        slide_content = []\n",
        "        slide_content.append(f\"Slide {i+1}\")\n",
        "\n",
        "        # Extract title\n",
        "        if slide.shapes.title:\n",
        "            title_text = slide.shapes.title.text\n",
        "            slide_content.append(f\"Title: {title_text}\")\n",
        "\n",
        "        # Extract text from all shapes\n",
        "        texts = []\n",
        "        for shape in slide.shapes:\n",
        "            if hasattr(shape, \"text\") and shape.text:\n",
        "                texts.append(shape.text)\n",
        "\n",
        "        if texts:\n",
        "            slide_content.append(\"Content:\")\n",
        "            for text in texts:\n",
        "                slide_content.append(text)\n",
        "\n",
        "        # Extract notes\n",
        "        if hasattr(slide, \"has_notes_slide\") and slide.has_notes_slide and slide.notes_slide.notes_text_frame.text:\n",
        "            notes = slide.notes_slide.notes_text_frame.text\n",
        "            slide_content.append(f\"Notes: {notes}\")\n",
        "\n",
        "        content.append(\"\\n\".join(slide_content))\n",
        "\n",
        "    return \"\\n\\n\".join(content)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T10:53:23.545378Z",
          "iopub.execute_input": "2025-04-22T10:53:23.545849Z",
          "iopub.status.idle": "2025-04-22T10:53:23.55254Z",
          "shell.execute_reply.started": "2025-04-22T10:53:23.545824Z",
          "shell.execute_reply": "2025-04-22T10:53:23.551811Z"
        },
        "id": "v2jpWYZEGKMd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_audio(video_path, audio_path):\n",
        "    \"\"\"Uses ffmpeg to extract mono 16kHz WAV audio.\"\"\"\n",
        "    try:\n",
        "        # Check if video file exists\n",
        "        if not os.path.exists(video_path):\n",
        "            print(f\"Video file does not exist: {video_path}\")\n",
        "            return False\n",
        "\n",
        "        # Make sure the output directory exists\n",
        "        os.makedirs(os.path.dirname(audio_path), exist_ok=True)\n",
        "\n",
        "        # Kaggle has ffmpeg pre-installed\n",
        "        print(f\"Running ffmpeg on {video_path}\")\n",
        "        command = f\"ffmpeg -i '{video_path}' -vn -acodec pcm_s16le -ar 16000 -ac 1 '{audio_path}' -y\"\n",
        "        result = subprocess.run(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "        # Check if the command was successful\n",
        "        if result.returncode != 0:\n",
        "            print(f\"ffmpeg error: {result.stderr.decode('utf-8')}\")\n",
        "            return False\n",
        "\n",
        "        # Verify the output file exists\n",
        "        if not os.path.exists(audio_path):\n",
        "            print(f\"Audio extraction failed: Output file not created\")\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error in audio extraction: {e}\")\n",
        "        return False\n",
        "\n",
        "\n",
        "# Function to transcribe audio\n",
        "def transcribe_audio(audio_path):\n",
        "    \"\"\"Transcribes audio file to text using Whisper.\"\"\"\n",
        "    try:\n",
        "        # Check if audio file exists\n",
        "        if not os.path.exists(audio_path):\n",
        "            print(f\"Audio file does not exist: {audio_path}\")\n",
        "            return \"[Transcription failed: Audio file not found]\"\n",
        "\n",
        "        print(f\"Transcribing {audio_path}...\")\n",
        "        result = asr(audio_path)\n",
        "        transcript = result.get(\"text\", \"\")\n",
        "\n",
        "        if not transcript:\n",
        "            return \"[Transcription produced no text]\"\n",
        "\n",
        "        return transcript\n",
        "    except Exception as e:\n",
        "        print(f\"Error in transcription: {e}\")\n",
        "        return f\"[Transcription error: {str(e)}]\"\n",
        "\n",
        "\n",
        "# Function to split text into chunks for LLM processing\n",
        "def chunk_text(text, max_chunk_size=8000, overlap=500):\n",
        "    \"\"\"Split text into chunks with overlap for processing by LLMs.\"\"\"\n",
        "    if len(text) <= max_chunk_size:\n",
        "        return [text]\n",
        "\n",
        "    chunks = []\n",
        "    start = 0\n",
        "\n",
        "    while start < len(text):\n",
        "        end = min(start + max_chunk_size, len(text))\n",
        "\n",
        "        # Try to find a good breaking point (newline)\n",
        "        if end < len(text):\n",
        "            # Look for a newline to break at\n",
        "            newline_pos = text.rfind('\\n', start + max_chunk_size - overlap, end)\n",
        "            if newline_pos != -1:\n",
        "                end = newline_pos + 1\n",
        "\n",
        "        chunks.append(text[start:end])\n",
        "        start = end - overlap if end < len(text) else end\n",
        "\n",
        "    return chunks\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T10:53:26.510508Z",
          "iopub.execute_input": "2025-04-22T10:53:26.511321Z",
          "iopub.status.idle": "2025-04-22T10:53:26.519558Z",
          "shell.execute_reply.started": "2025-04-22T10:53:26.511292Z",
          "shell.execute_reply": "2025-04-22T10:53:26.518927Z"
        },
        "id": "_hdeeCKhGKMd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "def generate_text(prompt):\n",
        "    \"\"\"Generates text based on the provided prompt using Gemini.\"\"\"\n",
        "    try:\n",
        "        generation_config = {\n",
        "            \"temperature\": 0.2,\n",
        "            \"top_p\": 0.95,\n",
        "            \"top_k\": 64,\n",
        "            \"max_output_tokens\": 8192,\n",
        "        }\n",
        "\n",
        "        response = model.generate_content(prompt, generation_config=generation_config)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating text: {e}\")\n",
        "        return f\"An error occurred: {e}\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T10:53:30.094882Z",
          "iopub.execute_input": "2025-04-22T10:53:30.095453Z",
          "iopub.status.idle": "2025-04-22T10:53:30.100288Z",
          "shell.execute_reply.started": "2025-04-22T10:53:30.09542Z",
          "shell.execute_reply": "2025-04-22T10:53:30.099554Z"
        },
        "id": "hczGOKWIGKMe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_notes(combined_text):\n",
        "    \"\"\"Generate structured lecture notes from combined content.\"\"\"\n",
        "    # Split text into manageable chunks if needed\n",
        "    chunks = chunk_text(combined_text)\n",
        "\n",
        "    if len(chunks) == 1:\n",
        "        # Simple case - content fits in one chunk\n",
        "        prompt = (\n",
        "            \"Create detailed and comprehensive lecture notes with the following structure:\\n\\n\"\n",
        "            \"1. DETAILED NOTES (thorough explanation of all main topics and subtopics)\\n\"\n",
        "            \"2. KEY POINTS (comprehensive list of important concepts with explanations)\\n\"\n",
        "            \"3. CONCEPTS & DEFINITIONS (detailed explanation of all technical terms and their applications)\\n\"\n",
        "            \"4. QUESTIONS & ANSWERS (5-6 in-depth Q&As covering complex topics)\\n\"\n",
        "            \"5. PRACTICAL EXAMPLES & APPLICATIONS (real-world examples and use cases)\\n\"\n",
        "            \"6. ADDITIONAL INSIGHTS (deeper analysis, related concepts, and important connections)\\n\\n\"\n",
        "            \"Source content:\\n\"\n",
        "            f\"{chunks[0]}\"\n",
        "        )\n",
        "        return generate_text(prompt)\n",
        "    else:\n",
        "        # Process each chunk and then combine\n",
        "        processed_chunks = []\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            print(f\"Processing chunk {i+1}/{len(chunks)}...\")\n",
        "            chunk_prompt = (\n",
        "                f\"This is part {i+1} of {len(chunks)} of a lecture. \"\n",
        "                \"Create detailed notes with these sections:\\n\"\n",
        "                \"1. DETAILED NOTES\\n\"\n",
        "                \"2. KEY POINTS\\n\"\n",
        "                \"3. CONCEPTS & DEFINITIONS\\n\"\n",
        "                \"4. QUESTIONS & ANSWERS\\n\"\n",
        "                \"5. PRACTICAL EXAMPLES & APPLICATIONS\\n\"\n",
        "                \"6. ADDITIONAL INSIGHTS\\n\\n\"\n",
        "                f\"Content:\\n{chunk}\"\n",
        "            )\n",
        "            processed_chunks.append(generate_text(chunk_prompt))\n",
        "\n",
        "        # Combine the processed chunks\n",
        "        combined_notes = \"\\n\\n\".join(processed_chunks)\n",
        "\n",
        "        # Final pass to ensure coherence\n",
        "        final_prompt = (\n",
        "            \"Combine these lecture notes into a single comprehensive document with:\\n\"\n",
        "            \"1. DETAILED NOTES (thorough explanation of all topics)\\n\"\n",
        "            \"2. KEY POINTS (comprehensive list with explanations)\\n\"\n",
        "            \"3. CONCEPTS & DEFINITIONS (detailed technical explanations)\\n\"\n",
        "            \"4. QUESTIONS & ANSWERS (in-depth Q&As)\\n\"\n",
        "            \"5. PRACTICAL EXAMPLES & APPLICATIONS\\n\"\n",
        "            \"6. ADDITIONAL INSIGHTS\\n\\n\"\n",
        "            \"Ensure thorough coverage of all topics and maintain logical flow between sections.\\n\\n\"\n",
        "            f\"{combined_notes}\"\n",
        "        )\n",
        "\n",
        "        return generate_text(final_prompt)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T10:53:32.993913Z",
          "iopub.execute_input": "2025-04-22T10:53:32.994442Z",
          "iopub.status.idle": "2025-04-22T10:53:33.000038Z",
          "shell.execute_reply.started": "2025-04-22T10:53:32.994417Z",
          "shell.execute_reply": "2025-04-22T10:53:32.999384Z"
        },
        "id": "3F8MVAeiGKMe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pdf(text, output_path):\n",
        "    \"\"\"Convert text to PDF format.\"\"\"\n",
        "    pdf = FPDF()\n",
        "    pdf.add_page()\n",
        "    pdf.set_auto_page_break(auto=True, margin=15)\n",
        "\n",
        "    # Set font - using Arial as it's widely available\n",
        "    pdf.set_font('Arial', size=11)\n",
        "\n",
        "    # Process text line by line\n",
        "    lines = text.split('\\n')\n",
        "    for line in lines:\n",
        "        # Check if line is a heading (simple heuristic)\n",
        "        if re.match(r'^#+\\s', line) or line.isupper() or (len(line) < 50 and line.strip().endswith(':')):\n",
        "            pdf.set_font('Arial', 'B', size=14)\n",
        "            pdf.cell(0, 10, line.strip(), ln=True)\n",
        "            pdf.set_font('Arial', size=11)\n",
        "        else:\n",
        "            # Handle regular text, wrapping as needed\n",
        "            pdf.multi_cell(0, 7, line)\n",
        "            pdf.ln(2)\n",
        "\n",
        "    # Save the PDF\n",
        "    try:\n",
        "        pdf.output(output_path)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating PDF: {e}\")\n",
        "        # Fallback to text file if PDF creation fails\n",
        "        with open(output_path.replace('.pdf', '.txt'), 'w', encoding='utf-8') as f:\n",
        "            f.write(text)\n",
        "        return False\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T10:53:36.4692Z",
          "iopub.execute_input": "2025-04-22T10:53:36.469679Z",
          "iopub.status.idle": "2025-04-22T10:53:36.475644Z",
          "shell.execute_reply.started": "2025-04-22T10:53:36.469656Z",
          "shell.execute_reply": "2025-04-22T10:53:36.474862Z"
        },
        "id": "SzINZeCmGKMe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def process_subfolder(subfolder_path):\n",
        "    \"\"\"Processes one subfolder: PDFs/PPTs → text, videos → transcript → notes.\"\"\"\n",
        "    parts = []\n",
        "    folder_name = os.path.basename(subfolder_path)\n",
        "    print(f\"\\nProcessing folder: {folder_name}\")\n",
        "\n",
        "    # 1. Extract from PowerPoint files (PPTX)\n",
        "    for fname in os.listdir(subfolder_path):\n",
        "        if fname.lower().endswith(\".pptx\"):\n",
        "            pptx_path = os.path.join(subfolder_path, fname)\n",
        "            print(f\"Extracting content from PowerPoint: {fname}\")\n",
        "            pptx_content = extract_pptx_content(pptx_path)\n",
        "            parts.append(f\"=== PowerPoint: {fname} ===\\n\" + pptx_content)\n",
        "\n",
        "    # 2. Extract from PDFs (as fallback or additional content)\n",
        "    for fname in os.listdir(subfolder_path):\n",
        "        if fname.lower().endswith(\".pdf\"):\n",
        "            pdf_path = os.path.join(subfolder_path, fname)\n",
        "            print(f\"Extracting content from PDF: {fname}\")\n",
        "            pdf_content = extract_pdf_text(pdf_path)\n",
        "            parts.append(f\"=== PDF: {fname} ===\\n\" + pdf_content)\n",
        "\n",
        "    # 3. Transcribe videos\n",
        "    for fname in os.listdir(subfolder_path):\n",
        "        if fname.lower().endswith((\".mp4\", \".mkv\", \".avi\", \".mov\")):\n",
        "            vid_path = os.path.join(subfolder_path, fname)\n",
        "            if not os.path.exists(vid_path):\n",
        "                print(f\"Warning: Video file not found: {vid_path}\")\n",
        "                continue\n",
        "\n",
        "            # Create audio path in the working directory\n",
        "            audio_path = os.path.join(\"/kaggle/working\", f\"{os.path.basename(subfolder_path)}_{os.path.basename(os.path.splitext(vid_path)[0])}.wav\")\n",
        "\n",
        "            try:\n",
        "                print(f\"Extracting audio from video: {fname}\")\n",
        "                success = extract_audio(vid_path, audio_path)\n",
        "\n",
        "                if not success or not os.path.exists(audio_path):\n",
        "                    print(f\"Warning: Failed to extract audio from {fname}\")\n",
        "                    continue\n",
        "\n",
        "                print(f\"Transcribing audio: {os.path.basename(audio_path)}\")\n",
        "                transcript = transcribe_audio(audio_path)\n",
        "                parts.append(f\"=== Transcript: {fname} ===\\n\" + transcript)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing video {fname}: {e}\")\n",
        "            finally:\n",
        "                # Clean up temporary audio file\n",
        "                if os.path.exists(audio_path):\n",
        "                    try:\n",
        "                        os.remove(audio_path)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Warning: Could not remove temporary audio file: {e}\")\n",
        "\n",
        "    # 4. Combine and generate notes\n",
        "    if not parts:\n",
        "        print(f\"No content found in {folder_name}. Skipping.\")\n",
        "        # Create a simple note indicating no content was found\n",
        "        notes = f\"No content was found in the folder '{folder_name}'. Please check that the folder contains PDF, PowerPoint, or video files.\"\n",
        "\n",
        "        # Create output directory in Kaggle's output folder\n",
        "        output_dir = \"/kaggle/working/lecture_notes\"\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        # Save a simple text file\n",
        "        txt_path = os.path.join(output_dir, f\"{folder_name}_no_content.txt\")\n",
        "        with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(notes)\n",
        "\n",
        "        print(f\"Created empty note file: {txt_path}\")\n",
        "        return False\n",
        "\n",
        "    print(f\"Combining content and generating notes for {folder_name}...\")\n",
        "    combined = \"\\n\\n\".join(parts)\n",
        "\n",
        "    try:\n",
        "        notes = generate_notes(combined)\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating notes: {e}\")\n",
        "        notes = f\"Error generating notes: {str(e)}\\n\\nRaw content:\\n\\n{combined[:1000]}...\"\n",
        "\n",
        "    # Create output directory in Kaggle's output folder\n",
        "    output_dir = \"/kaggle/working/lecture_notes\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # 5. Save to PDF file\n",
        "    out_path = os.path.join(output_dir, f\"{folder_name}_lecture_notes.pdf\")\n",
        "    print(f\"Saving notes to {out_path}\")\n",
        "\n",
        "    pdf_success = create_pdf(notes, out_path)\n",
        "\n",
        "    # Also save as text file for backup\n",
        "    txt_path = os.path.join(output_dir, f\"{folder_name}_lecture_notes.txt\")\n",
        "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(notes)\n",
        "\n",
        "    print(f\"Notes saved: {out_path if pdf_success else txt_path}\")\n",
        "\n",
        "    # Display a preview in the notebook\n",
        "    display(Markdown(f\"## Notes Preview for {folder_name}\"))\n",
        "    display(Markdown(notes[:1000] + \"...\\n\\n[See full notes in the output files]\"))\n",
        "\n",
        "    return True"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T10:53:40.702378Z",
          "iopub.execute_input": "2025-04-22T10:53:40.703248Z",
          "iopub.status.idle": "2025-04-22T10:53:40.714682Z",
          "shell.execute_reply.started": "2025-04-22T10:53:40.70322Z",
          "shell.execute_reply": "2025-04-22T10:53:40.713934Z"
        },
        "id": "cc-86wULGKMe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def main(root_folder):\n",
        "    \"\"\"Process all subfolders inside the root folder to generate lecture notes.\"\"\"\n",
        "    if not os.path.exists(root_folder):\n",
        "        print(f\"Error: Root folder '{root_folder}' not found.\")\n",
        "        return False\n",
        "\n",
        "    root = Path(root_folder)\n",
        "    subfolders = [f for f in root.iterdir() if f.is_dir()]\n",
        "\n",
        "    if not subfolders:\n",
        "        print(f\"No subfolders found in '{root_folder}'\")\n",
        "        return False\n",
        "\n",
        "    print(f\"Found {len(subfolders)} topics in '{root_folder}'\")\n",
        "\n",
        "    for subfolder in subfolders:\n",
        "        print(f\"\\n--- Processing folder: {subfolder.name} ---\")\n",
        "        success = process_subfolder(str(subfolder))\n",
        "\n",
        "        if success:\n",
        "            print(f\"[✓] Completed: {subfolder.name}\")\n",
        "        else:\n",
        "            print(f\"[✗] Failed: {subfolder.name}\")\n",
        "\n",
        "    print(\"\\n✅ All folders processed.\")\n",
        "    return True\n",
        "\n",
        "main(\"/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-22T10:55:08.986342Z",
          "iopub.execute_input": "2025-04-22T10:55:08.986633Z",
          "iopub.status.idle": "2025-04-22T10:59:08.198992Z",
          "shell.execute_reply.started": "2025-04-22T10:55:08.986595Z",
          "shell.execute_reply": "2025-04-22T10:59:08.19825Z"
        },
        "id": "-HyLr-lrGKMf",
        "outputId": "003bafca-9d35-4920-df7c-2caf0e72cf78"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Found 10 topics in '/kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET'\n\n--- Processing folder: stable_diffusion ---\n\nProcessing folder: stable_diffusion\nExtracting content from PDF: Stable Diffusion.pdf\nExtracting audio from video: 19853_shylaja.sharath_31_20250327092700214_Video_ENC (1).mp4\nRunning ffmpeg on /kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/stable_diffusion/19853_shylaja.sharath_31_20250327092700214_Video_ENC (1).mp4\nError in audio extraction: name 'subprocess' is not defined\nWarning: Failed to extract audio from 19853_shylaja.sharath_31_20250327092700214_Video_ENC (1).mp4\nExtracting audio from video: 19853_shylaja.sharath_31_20250327084200249_Video_ENC.mp4\nRunning ffmpeg on /kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/stable_diffusion/19853_shylaja.sharath_31_20250327084200249_Video_ENC.mp4\nError in audio extraction: name 'subprocess' is not defined\nWarning: Failed to extract audio from 19853_shylaja.sharath_31_20250327084200249_Video_ENC.mp4\nCombining content and generating notes for stable_diffusion...\nProcessing chunk 1/3...\nProcessing chunk 2/3...\nProcessing chunk 3/3...\nSaving notes to /kaggle/working/lecture_notes/stable_diffusion_lecture_notes.pdf\nNotes saved: /kaggle/working/lecture_notes/stable_diffusion_lecture_notes.pdf\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "## Notes Preview for stable_diffusion"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "# Stable Diffusion: A Comprehensive Guide\n\nThis document combines lecture notes on Stable Diffusion, providing a detailed overview of its functionality, underlying concepts, and advanced techniques.\n\n## 1. Detailed Notes\n\nStable Diffusion is a powerful latent diffusion model capable of generating high-quality images from text prompts.  Its efficiency stems from operating in a latent space, a lower-dimensional representation of images achieved through a Variational Autoencoder (VAE). This significantly reduces computational costs compared to models working directly in the high-dimensional image space.\n\n**A. Diffusion Models:** The core of Stable Diffusion lies in its diffusion process:\n\n* **Forward Diffusion:**  Iteratively adds Gaussian noise to an image, transforming it into pure noise. This process is deterministic and learned during training.\n* **Reverse Diffusion:** The model learns to reverse this process, starting from pure noise and iteratively removing noise guided by a text pr...\n\n[See full notes in the output files]"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "[✓] Completed: stable_diffusion\n\n--- Processing folder: expn_tree ---\n\nProcessing folder: expn_tree\nExtracting content from PowerPoint: Class7_Unit3_Trees_ExprTree.pptx\nExtracting audio from video: 7b_2020-09-25 12-12-37_ExprTReeCode 00_00_09-00_26_52.mkv\nRunning ffmpeg on /kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/expn_tree/7b_2020-09-25 12-12-37_ExprTReeCode 00_00_09-00_26_52.mkv\nError in audio extraction: name 'subprocess' is not defined\nWarning: Failed to extract audio from 7b_2020-09-25 12-12-37_ExprTReeCode 00_00_09-00_26_52.mkv\nExtracting audio from video: 7a_2020-09-24 09-28-52_ExprTreeCon.mkv\nRunning ffmpeg on /kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/expn_tree/7a_2020-09-24 09-28-52_ExprTreeCon.mkv\nError in audio extraction: name 'subprocess' is not defined\nWarning: Failed to extract audio from 7a_2020-09-24 09-28-52_ExprTreeCon.mkv\nCombining content and generating notes for expn_tree...\nProcessing chunk 1/2...\nProcessing chunk 2/2...\nSaving notes to /kaggle/working/lecture_notes/expn_tree_lecture_notes.pdf\nNotes saved: /kaggle/working/lecture_notes/expn_tree_lecture_notes.pdf\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "## Notes Preview for expn_tree"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "## Comprehensive Lecture Notes: Expression Trees\n\nThis document combines and expands upon the provided lecture notes on expression trees, offering a comprehensive overview of the topic.\n\n**1. DETAILED NOTES:**\n\nExpression trees are a fundamental data structure in computer science used to represent and evaluate arithmetic and logical expressions.  They offer a structured and efficient way to handle the complexities of operator precedence and parentheses.  This document covers two primary aspects: construction from postfix notation and evaluation.\n\n**1.1 Construction from Postfix Notation:**\n\nThe most efficient way to construct an expression tree is from a postfix (Reverse Polish Notation or RPN) expression.  Postfix notation eliminates the need for parentheses and explicit operator precedence rules, simplifying the construction algorithm.  The algorithm utilizes a stack:\n\n1. **Initialization:** Start with an empty stack.\n2. **Scan the Postfix Expression:** Process the postfix expression...\n\n[See full notes in the output files]"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "[✓] Completed: expn_tree\n\n--- Processing folder: agentic ---\n\nProcessing folder: agentic\nExtracting content from PDF: AutoGen, CrewAI.pdf\nExtracting content from PDF: Agentic Workflow.pdf\nExtracting audio from video: 19853_shylaja.sharath_31_20250401121200417_Video_ENC.mp4\nRunning ffmpeg on /kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/agentic/19853_shylaja.sharath_31_20250401121200417_Video_ENC.mp4\nError in audio extraction: name 'subprocess' is not defined\nWarning: Failed to extract audio from 19853_shylaja.sharath_31_20250401121200417_Video_ENC.mp4\nCombining content and generating notes for agentic...\nProcessing chunk 1/3...\nProcessing chunk 2/3...\nProcessing chunk 3/3...\nSaving notes to /kaggle/working/lecture_notes/agentic_lecture_notes.pdf\nNotes saved: /kaggle/working/lecture_notes/agentic_lecture_notes.pdf\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "## Notes Preview for agentic"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "## Comprehensive Lecture Notes: AutoGen, CrewAI, Agentic Workflows, and the Future of Intelligent Systems\n\nThis document combines the three lecture parts into a single, comprehensive resource covering AutoGen, CrewAI, agentic workflows, design patterns, visual AI, and the future of intelligent systems.\n\n**1. DETAILED NOTES:**\n\nThe lectures introduce AutoGen and CrewAI, two frameworks for building multi-agent systems using Large Language Models (LLMs), and then expand on the broader concept of agentic workflows and their associated design patterns.  Finally, the future implications of these advancements, particularly in visual AI, are explored.\n\n**Part A: AutoGen and CrewAI**\n\nAutoGen and CrewAI are frameworks for creating multi-agent systems.  AutoGen emphasizes flexible, conversational workflows where agents (AI or human) communicate via structured messages, iteratively refining tasks.  It supports integration with external tools and APIs.  CrewAI, conversely, focuses on modular agent...\n\n[See full notes in the output files]"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "[✓] Completed: agentic\n\n--- Processing folder: Lora_Qlora ---\n\nProcessing folder: Lora_Qlora\nExtracting content from PDF: Finetuning.pdf\nExtracting audio from video: 19853_shylaja.sharath_31_20250318125700082_Video_ENC.mp4\nRunning ffmpeg on /kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Lora_Qlora/19853_shylaja.sharath_31_20250318125700082_Video_ENC.mp4\nError in audio extraction: name 'subprocess' is not defined\nWarning: Failed to extract audio from 19853_shylaja.sharath_31_20250318125700082_Video_ENC.mp4\nExtracting audio from video: 19853_shylaja.sharath_31_20250318121200085_Video_ENC (1).mp4\nRunning ffmpeg on /kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Lora_Qlora/19853_shylaja.sharath_31_20250318121200085_Video_ENC (1).mp4\nError in audio extraction: name 'subprocess' is not defined\nWarning: Failed to extract audio from 19853_shylaja.sharath_31_20250318121200085_Video_ENC (1).mp4\nExtracting audio from video: 19853_shylaja.sharath_31_20250318112700094_Video_ENC (1).mp4\nRunning ffmpeg on /kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Lora_Qlora/19853_shylaja.sharath_31_20250318112700094_Video_ENC (1).mp4\nError in audio extraction: name 'subprocess' is not defined\nWarning: Failed to extract audio from 19853_shylaja.sharath_31_20250318112700094_Video_ENC (1).mp4\nCombining content and generating notes for Lora_Qlora...\nProcessing chunk 1/2...\nProcessing chunk 2/2...\nSaving notes to /kaggle/working/lecture_notes/Lora_Qlora_lecture_notes.pdf\nError creating PDF: 'latin-1' codec can't encode character '\\u2013' in position 1190: ordinal not in range(256)\nNotes saved: /kaggle/working/lecture_notes/Lora_Qlora_lecture_notes.txt\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "## Notes Preview for Lora_Qlora"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "## Fine-tuning Large Language Models: A Comprehensive Guide\n\nThis document combines and expands upon the provided lecture notes on fine-tuning Large Language Models (LLMs), offering a detailed and comprehensive overview of the subject.\n\n**1. DETAILED NOTES:**\n\nFine-tuning is a crucial technique for adapting pre-trained LLMs, like GPT, to specific tasks and domains.  Pre-trained LLMs, while powerful, often lack optimal performance for niche applications due to their general-purpose training on massive datasets. Fine-tuning addresses this limitation by making targeted adjustments to the model's parameters, transforming a generalist model into a specialist. This process leverages transfer learning, efficiently utilizing the pre-existing knowledge embedded within the LLM.\n\nThe general steps involved in fine-tuning are:\n\n1. **Dataset Preparation:** This crucial step involves gathering and meticulously preparing a dataset relevant to the target task.  Data cleaning, preprocessing (e.g., remo...\n\n[See full notes in the output files]"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "[✓] Completed: Lora_Qlora\n\n--- Processing folder: Heap ---\n\nProcessing folder: Heap\nExtracting content from PowerPoint: Class8_Unit3_Trees_Heap.pptx\nExtracting audio from video: 8a_2020-09-24 13-07-04_HeapCon 00_00_01-00_42_12~1.mkv\nRunning ffmpeg on /kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Heap/8a_2020-09-24 13-07-04_HeapCon 00_00_01-00_42_12~1.mkv\nError in audio extraction: name 'subprocess' is not defined\nWarning: Failed to extract audio from 8a_2020-09-24 13-07-04_HeapCon 00_00_01-00_42_12~1.mkv\nExtracting audio from video: 8b_2020-09-26 09-34-27_HeapCode 00_00_09-00_56_50.mkv\nRunning ffmpeg on /kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Heap/8b_2020-09-26 09-34-27_HeapCode 00_00_09-00_56_50.mkv\nError in audio extraction: name 'subprocess' is not defined\nWarning: Failed to extract audio from 8b_2020-09-26 09-34-27_HeapCode 00_00_09-00_56_50.mkv\nCombining content and generating notes for Heap...\nSaving notes to /kaggle/working/lecture_notes/Heap_lecture_notes.pdf\nError creating PDF: 'latin-1' codec can't encode character '\\u230a' in position 1308: ordinal not in range(256)\nNotes saved: /kaggle/working/lecture_notes/Heap_lecture_notes.txt\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "## Notes Preview for Heap"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "## Lecture Notes: Heap Data Structure\n\n**1. DETAILED NOTES**\n\nThis lecture covers heap data structures, their properties, and two construction methods: bottom-up and top-down.\n\n**1.1 Heap Definition:** A heap is a specialized binary tree satisfying two crucial properties:\n\n* **Shape Requirement:** It's an *essentially complete* binary tree.  All levels are full except possibly the last, where nodes are filled from left to right.\n* **Parental Dominance Requirement:** The key (value) of each node is greater than or equal to the keys of its children. This creates a max-heap; a min-heap would have the parent's key less than or equal to its children's.\n\n**1.2 Heap Properties:**\n\n* Uniqueness: For a given number of nodes (n), there's only one essentially complete binary tree. Its height is ⌊log₂n⌋.\n* Root Element: The root always contains the largest element (in a max-heap).\n* Sub-heap Property: Any node considered with its descendants forms a heap itself.\n* Array Representation: Heaps are e...\n\n[See full notes in the output files]"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "[✓] Completed: Heap\n\n--- Processing folder: BST ---\n\nProcessing folder: BST\nExtracting content from PowerPoint: Class2_Unit3_Tree_BST_DynamicInsert.pptx\nExtracting content from PowerPoint: Class3_Unit3_Trees_BSTDeletion.pptx\nExtracting content from PowerPoint: Class4_Unit3_Trees_BST_ArrayInsert.pptx\nCombining content and generating notes for BST...\nSaving notes to /kaggle/working/lecture_notes/BST_lecture_notes.pdf\nNotes saved: /kaggle/working/lecture_notes/BST_lecture_notes.pdf\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "## Notes Preview for BST"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "## Lecture Notes: Binary Search Trees (BST) - Implementation and Operations\n\n**1. DETAILED NOTES:**\n\nThis lecture covers Binary Search Trees (BSTs), focusing on their implementation using dynamic allocation (linked lists) and arrays, along with insertion and deletion operations.\n\n**1.1 Introduction to BSTs:**\n\nA BST is a binary tree where each node holds a value, and for every node:\n* All values in its left subtree are less than its value.\n* All values in its right subtree are greater than or equal to its value.\n\nThis property allows for efficient searching, insertion, and deletion operations with a time complexity of O(log n) in the average case, where n is the number of nodes.  However, in the worst-case scenario (e.g., a skewed tree), the complexity degrades to O(n).\n\n**1.2 BST Implementation using Dynamic Allocation (Linked Lists):**\n\nThis approach uses a `struct` (or class) to represent each node, containing the data (`info`), a pointer to the left child (`left`), and a pointer to...\n\n[See full notes in the output files]"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "[✓] Completed: BST\n\n--- Processing folder: TBT ---\n\nProcessing folder: TBT\nExtracting content from PowerPoint: Class6_Unit3_Trees_ThreadBST.pptx\nExtracting audio from video: 6b_2020-09-25 11-29-19_TBTCode 00_00_04-00_35_19~8.mkv\nRunning ffmpeg on /kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/TBT/6b_2020-09-25 11-29-19_TBTCode 00_00_04-00_35_19~8.mkv\nError in audio extraction: name 'subprocess' is not defined\nWarning: Failed to extract audio from 6b_2020-09-25 11-29-19_TBTCode 00_00_04-00_35_19~8.mkv\nExtracting audio from video: 6a_2020-09-22 09-49-32_TBTCon.mkv\nRunning ffmpeg on /kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/TBT/6a_2020-09-22 09-49-32_TBTCon.mkv\nError in audio extraction: name 'subprocess' is not defined\nWarning: Failed to extract audio from 6a_2020-09-22 09-49-32_TBTCon.mkv\nCombining content and generating notes for TBT...\nSaving notes to /kaggle/working/lecture_notes/TBT_lecture_notes.pdf\nNotes saved: /kaggle/working/lecture_notes/TBT_lecture_notes.pdf\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "## Notes Preview for TBT"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "## Lecture Notes: Threaded Binary Search Trees\n\n**1. DETAILED NOTES**\n\nThis lecture explores Threaded Binary Search Trees (BSTs), a variation of binary search trees designed to optimize inorder traversal.  Standard inorder traversal using recursion or an explicit stack can be inefficient, especially for large trees. Threaded BSTs address this by modifying the tree structure to implicitly store inorder predecessor and/or successor information within the nodes themselves, eliminating the need for an external stack.\n\n**1.1 Motivation:**  The primary motivation behind threaded BSTs is to improve the efficiency of inorder traversal.  Standard inorder traversal methods (recursive or iterative with a stack) incur overhead due to function calls (recursion) or stack management (iteration).  Threaded BSTs aim to remove this overhead by using the unused pointers in nodes to point to inorder predecessors and successors.\n\n**1.2 Types of Threaded BSTs:**\n\n* **Right-In Threaded BST:**  The `right` po...\n\n[See full notes in the output files]"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "[✓] Completed: TBT\n\n--- Processing folder: Multimodal ---\n\nProcessing folder: Multimodal\nExtracting content from PDF: MAMBA.pdf\nExtracting content from PDF: MultiModal LLMs.pdf\nExtracting audio from video: 19853_shylaja.sharath_31_20250401112700078_Video_ENC.mp4\nRunning ffmpeg on /kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Multimodal/19853_shylaja.sharath_31_20250401112700078_Video_ENC.mp4\nError in audio extraction: name 'subprocess' is not defined\nWarning: Failed to extract audio from 19853_shylaja.sharath_31_20250401112700078_Video_ENC.mp4\nCombining content and generating notes for Multimodal...\nProcessing chunk 1/4...\nProcessing chunk 2/4...\nProcessing chunk 3/4...\nProcessing chunk 4/4...\nSaving notes to /kaggle/working/lecture_notes/Multimodal_lecture_notes.pdf\nError creating PDF: 'latin-1' codec can't encode character '\\u2013' in position 1330: ordinal not in range(256)\nNotes saved: /kaggle/working/lecture_notes/Multimodal_lecture_notes.txt\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "## Notes Preview for Multimodal"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "## Mamba and Multimodal Large Language Models: A Comprehensive Overview\n\nThis document combines lecture notes on Mamba, a novel neural network architecture, and Multimodal Large Language Models (MLLMs), providing a detailed and comprehensive understanding of both topics.\n\n**I. Mamba: A Novel Neural Network Architecture**\n\n**1. Detailed Notes:**\n\nMamba is a novel neural network architecture designed for efficient sequence processing (text, audio, DNA, etc.).  Unlike Transformers, which rely on computationally expensive attention mechanisms, Mamba employs Selective State Space Models (SSMs).  SSMs maintain an internal state, updating it sequentially as it processes input.  The \"selectivity\" allows Mamba to prioritize relevant information, discarding irrelevant details, mimicking attention's functionality with significantly reduced computational cost.  This is analogous to following a recipe – remembering progress without rereading the entire recipe at each step.  Email filtering, priorit...\n\n[See full notes in the output files]"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "[✓] Completed: Multimodal\n\n--- Processing folder: binary_tree_traversal ---\n\nProcessing folder: binary_tree_traversal\nExtracting content from PowerPoint: Class5_Unit3_BST_Traversal.pptx\nExtracting audio from video: 5a_2020-09-15 09-04-51_BinTraversal.mkv\nRunning ffmpeg on /kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/binary_tree_traversal/5a_2020-09-15 09-04-51_BinTraversal.mkv\nError in audio extraction: name 'subprocess' is not defined\nWarning: Failed to extract audio from 5a_2020-09-15 09-04-51_BinTraversal.mkv\nCombining content and generating notes for binary_tree_traversal...\nProcessing chunk 1/3...\nProcessing chunk 2/3...\nProcessing chunk 3/3...\nSaving notes to /kaggle/working/lecture_notes/binary_tree_traversal_lecture_notes.pdf\nNotes saved: /kaggle/working/lecture_notes/binary_tree_traversal_lecture_notes.pdf\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "## Notes Preview for binary_tree_traversal"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "# Binary Search Tree Traversal: A Comprehensive Guide\n\nThis document consolidates the lecture notes on Binary Search Tree Traversal, providing a detailed explanation of preorder, inorder, and postorder traversals, with a focus on iterative implementations using stacks.\n\n## 1. Detailed Notes\n\nThis lecture series covers three standard binary tree traversals: preorder, inorder, and postorder.  We explore both recursive definitions and iterative implementations (using stacks) for inorder and preorder, with a detailed step-by-step analysis of iterative inorder traversal.  The iterative postorder traversal, requiring two stacks, is also explained in detail.  The examples consistently use a sample binary tree to illustrate the traversal order and the stack's role in iterative algorithms.\n\n**Part 1:** Introduces the concept of binary tree traversal, defining preorder (VLR), inorder (LVR), and postorder (LRV) traversals recursively.  A detailed, step-by-step illustration of iterative inorder tr...\n\n[See full notes in the output files]"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "[✓] Completed: binary_tree_traversal\n\n--- Processing folder: Tree_traversal ---\n\nProcessing folder: Tree_traversal\nExtracting content from PowerPoint: class9_Unit3_Trees_naryTraversal.pptx\nExtracting audio from video: 9a_2020-09-16 09-46-13_TreeTravCon.mkv\nRunning ffmpeg on /kaggle/input/llm-dataset/LLM_DATASET/LLM_DATASET/Tree_traversal/9a_2020-09-16 09-46-13_TreeTravCon.mkv\nError in audio extraction: name 'subprocess' is not defined\nWarning: Failed to extract audio from 9a_2020-09-16 09-46-13_TreeTravCon.mkv\nCombining content and generating notes for Tree_traversal...\nSaving notes to /kaggle/working/lecture_notes/Tree_traversal_lecture_notes.pdf\nNotes saved: /kaggle/working/lecture_notes/Tree_traversal_lecture_notes.pdf\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "## Notes Preview for Tree_traversal"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "## Lecture Notes: N-ary Tree Traversal\n\n**1. DETAILED NOTES:**\n\nThis lecture covers tree traversal techniques specifically for n-ary trees, where each node can have multiple children.  Unlike binary trees, n-ary trees utilize two pointers within their node structure: `child` (pointing to the first child) and `sibling` (pointing to the next sibling).\n\n**1.1 Tree Node Structure:**\n\nThe fundamental building block is the `treenode` structure:\n\n```c\nstruct treenode {\n    int info; // Data stored in the node\n    struct treenode *child; // Pointer to the first child\n    struct treenode *sibling; // Pointer to the next sibling\n};\n```\n\n**1.2 Tree Traversal Algorithms:**\n\nThree primary traversal methods exist for n-ary trees: preorder, inorder, and postorder.  These differ in the order they visit the root node relative to its children and siblings.\n\n**1.2.1 Preorder Traversal:**\n\nThe algorithm follows these steps:\n\n1. Visit the root node.\n2. Recursively traverse the children (forming a sub-fores...\n\n[See full notes in the output files]"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "[✓] Completed: Tree_traversal\n\n✅ All folders processed.\n",
          "output_type": "stream"
        },
        {
          "execution_count": 14,
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "XAmhdlsYGKMf"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}